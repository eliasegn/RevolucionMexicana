{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Extracción de Entidades\n",
        "\n",
        "Este notebook es para hacer extracción de entidades a los documentos. Aquí se usó el modelo \"mrm8488/bert-spanish-cased-finetuned-ner\" de Hugging Face, el cual fue de mucha utilidad"
      ],
      "metadata": {
        "id": "9bSS3pZneCgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36GGhu7reQW1",
        "outputId": "963ef1b7-902b-41d9-8dd0-302dadde9ac9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividimos nuestro texto en máximo 400 palabras\n",
        "def dividir_texto(texto, max_palabras=400):\n",
        "    palabras = texto.split()\n",
        "    return [\" \".join(palabras[i:i+max_palabras]) for i in range(0, len(palabras), max_palabras)]\n",
        "\n",
        "# Hacemos extracción de entidades\n",
        "def extraer_entidades(gen, libro, n):\n",
        "\n",
        "  entidades_extraidas = pd.DataFrame()\n",
        "  ner_pipeline = pipeline(\"ner\", model=\"mrm8488/bert-spanish-cased-finetuned-ner\", grouped_entities=True)\n",
        "\n",
        "  for i in range(1, n+1):\n",
        "    ruta = f\"/content/drive/MyDrive/Proyecto_Text2KG/Texto/gen{gen}libro{libro}_carpetatxt/gen{gen}libro{libro}_page-{i:04d}.txt\"\n",
        "\n",
        "    if not os.path.exists(ruta):\n",
        "        print(f\"Archivo no encontrado: {ruta}\")\n",
        "        continue\n",
        "\n",
        "    with open(ruta, \"r\", encoding=\"utf-8\") as f:\n",
        "      texto = f.read()\n",
        "\n",
        "    # Dividir el texto en fragmentos pequeños\n",
        "    fragmentos = dividir_texto(texto, max_palabras=400)\n",
        "\n",
        "    for fragmento in fragmentos:\n",
        "      try:\n",
        "        entidades = ner_pipeline(fragmento)\n",
        "      except Exception as e:\n",
        "        print(f\"Error procesando página {i}, fragmento omitido: {e}\")\n",
        "        continue\n",
        "\n",
        "      for entidad in entidades:\n",
        "        nueva_fila = {\n",
        "            \"generacion\": gen,\n",
        "            \"libro\": libro,\n",
        "            \"pagina\": i,\n",
        "            \"entidad\": entidad[\"word\"],\n",
        "            \"tipo\": entidad[\"entity_group\"],\n",
        "            \"score\": entidad[\"score\"]\n",
        "        }\n",
        "        entidades_extraidas = pd.concat([entidades_extraidas, pd.DataFrame([nueva_fila])], ignore_index=True)\n",
        "\n",
        "  return entidades_extraidas\n"
      ],
      "metadata": {
        "id": "zHQsav-ofOqj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora extraeremos las entidades de todos los libros de cada generación para hacer extracción de relaciones y haremos archivos csv para visualización."
      ],
      "metadata": {
        "id": "WVYjCBP2doTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g1960l2 = extraer_entidades(1960, 2, 9)\n",
        "g1960l2.to_csv(\"/content/drive/MyDrive/Proyecto_Text2KG/Texto/g1960l2.csv\", index=False)\n",
        "\n",
        "g1960l3 = extraer_entidades(1960, 3, 24)\n",
        "g1960l3.to_csv(\"/content/drive/MyDrive/Proyecto_Text2KG/Texto/g1960l3.csv\", index=False)\n",
        "\n",
        "g1962l1 = extraer_entidades(1962, 1, 11)\n",
        "g1962l1.to_csv(\"/content/drive/MyDrive/Proyecto_Text2KG/Texto/g1962l1.csv\", index=False)\n",
        "\n",
        "g1962l2 = extraer_entidades(1962, 2, 24)\n",
        "g1962l2.to_csv(\"/content/drive/MyDrive/Proyecto_Text2KG/Texto/g1962l2.csv\", index=False)\n",
        "\n",
        "g1972l1 = extraer_entidades(1972, 1, 4)\n",
        "g1972l1.to_csv(\"/content/drive/MyDrive/Proyecto_Text2KG/Texto/g1972l1.csv\", index=False)\n",
        "\n",
        "g1982l1 = extraer_entidades(1982, 1, 5)\n",
        "g1982l1.to_csv(\"/content/drive/MyDrive/Proyecto_Text2KG/Texto/g1982l1.csv\", index=False)\n",
        "\n",
        "g1993l1 = extraer_entidades(1993, 1, 45)\n",
        "g1993l1.to_csv(\"/content/drive/MyDrive/Proyecto_Text2KG/Texto/g1993l1.csv\", index=False)\n",
        "\n",
        "g1993l2 = extraer_entidades(1993, 2, 24)\n",
        "g1993l2.to_csv(\"/content/drive/MyDrive/Proyecto_Text2KG/Texto/g1993l2.csv\", index=False)\n",
        "\n",
        "g2008l1 = extraer_entidades(2008, 1, 23)\n",
        "g2008l1.to_csv(\"/content/drive/MyDrive/Proyecto_Text2KG/Texto/g2008l1.csv\", index=False)\n",
        "\n",
        "g2008l2 = extraer_entidades(2008, 2, 36)\n",
        "g2008l2.to_csv(\"/content/drive/MyDrive/Proyecto_Text2KG/Texto/g2008l2.csv\", index=False)\n",
        "\n",
        "g2011l1 = extraer_entidades(2011, 1, 67)\n",
        "g2011l1.to_csv(\"/content/drive/MyDrive/Proyecto_Text2KG/Texto/g2011l1.csv\", index=False)\n",
        "\n",
        "g2014l1 = extraer_entidades(2014, 1, 71)\n",
        "g2014l1.to_csv(\"/content/drive/MyDrive/Proyecto_Text2KG/Texto/g2014l1.csv\", index=False)\n",
        "\n",
        "g2019l1 = extraer_entidades(2019, 1, 71)\n",
        "g2019l1.to_csv(\"/content/drive/MyDrive/Proyecto_Text2KG/Texto/g2019l1.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e61wYREc8KJ",
        "outputId": "084b8a01-f356-43d7-9475-55b6c41e5b92"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at mrm8488/bert-spanish-cased-finetuned-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
            "  warnings.warn(\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Some weights of the model checkpoint at mrm8488/bert-spanish-cased-finetuned-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
            "  warnings.warn(\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error procesando página 14, fragmento omitido: The size of tensor a (528) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Error procesando página 22, fragmento omitido: The size of tensor a (600) must match the size of tensor b (512) at non-singleton dimension 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at mrm8488/bert-spanish-cased-finetuned-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
            "  warnings.warn(\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Some weights of the model checkpoint at mrm8488/bert-spanish-cased-finetuned-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
            "  warnings.warn(\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error procesando página 3, fragmento omitido: The size of tensor a (521) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Error procesando página 16, fragmento omitido: The size of tensor a (531) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Error procesando página 19, fragmento omitido: The size of tensor a (524) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Error procesando página 21, fragmento omitido: The size of tensor a (520) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Error procesando página 22, fragmento omitido: The size of tensor a (581) must match the size of tensor b (512) at non-singleton dimension 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at mrm8488/bert-spanish-cased-finetuned-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
            "  warnings.warn(\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Some weights of the model checkpoint at mrm8488/bert-spanish-cased-finetuned-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
            "  warnings.warn(\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error procesando página 2, fragmento omitido: The size of tensor a (558) must match the size of tensor b (512) at non-singleton dimension 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at mrm8488/bert-spanish-cased-finetuned-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
            "  warnings.warn(\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen1993libro1_carpetatxt/gen1993libro1_page-0002.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen1993libro1_carpetatxt/gen1993libro1_page-0004.txt\n",
            "Error procesando página 5, fragmento omitido: The size of tensor a (551) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen1993libro1_carpetatxt/gen1993libro1_page-0006.txt\n",
            "Error procesando página 7, fragmento omitido: The size of tensor a (575) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen1993libro1_carpetatxt/gen1993libro1_page-0008.txt\n",
            "Error procesando página 9, fragmento omitido: The size of tensor a (569) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen1993libro1_carpetatxt/gen1993libro1_page-0010.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen1993libro1_carpetatxt/gen1993libro1_page-0012.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen1993libro1_carpetatxt/gen1993libro1_page-0014.txt\n",
            "Error procesando página 15, fragmento omitido: The size of tensor a (612) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen1993libro1_carpetatxt/gen1993libro1_page-0016.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen1993libro1_carpetatxt/gen1993libro1_page-0018.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen1993libro1_carpetatxt/gen1993libro1_page-0020.txt\n",
            "Error procesando página 21, fragmento omitido: The size of tensor a (602) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen1993libro1_carpetatxt/gen1993libro1_page-0022.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen1993libro1_carpetatxt/gen1993libro1_page-0024.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen1993libro1_carpetatxt/gen1993libro1_page-0026.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen1993libro1_carpetatxt/gen1993libro1_page-0028.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen1993libro1_carpetatxt/gen1993libro1_page-0030.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen1993libro1_carpetatxt/gen1993libro1_page-0032.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen1993libro1_carpetatxt/gen1993libro1_page-0034.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen1993libro1_carpetatxt/gen1993libro1_page-0036.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen1993libro1_carpetatxt/gen1993libro1_page-0038.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen1993libro1_carpetatxt/gen1993libro1_page-0040.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen1993libro1_carpetatxt/gen1993libro1_page-0042.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen1993libro1_carpetatxt/gen1993libro1_page-0044.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at mrm8488/bert-spanish-cased-finetuned-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
            "  warnings.warn(\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error procesando página 1, fragmento omitido: The size of tensor a (575) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Error procesando página 3, fragmento omitido: The size of tensor a (555) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Error procesando página 7, fragmento omitido: The size of tensor a (530) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Error procesando página 10, fragmento omitido: The size of tensor a (639) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Error procesando página 11, fragmento omitido: The size of tensor a (578) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Error procesando página 12, fragmento omitido: The size of tensor a (589) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Error procesando página 14, fragmento omitido: The size of tensor a (589) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Error procesando página 15, fragmento omitido: The size of tensor a (608) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Error procesando página 18, fragmento omitido: The size of tensor a (588) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Error procesando página 21, fragmento omitido: The size of tensor a (585) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Error procesando página 22, fragmento omitido: The size of tensor a (588) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Error procesando página 23, fragmento omitido: The size of tensor a (537) must match the size of tensor b (512) at non-singleton dimension 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at mrm8488/bert-spanish-cased-finetuned-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
            "  warnings.warn(\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error procesando página 6, fragmento omitido: The size of tensor a (556) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Error procesando página 7, fragmento omitido: The size of tensor a (555) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Error procesando página 8, fragmento omitido: The size of tensor a (551) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Error procesando página 11, fragmento omitido: The size of tensor a (605) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Error procesando página 14, fragmento omitido: The size of tensor a (575) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Error procesando página 22, fragmento omitido: The size of tensor a (540) must match the size of tensor b (512) at non-singleton dimension 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at mrm8488/bert-spanish-cased-finetuned-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
            "  warnings.warn(\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error procesando página 22, fragmento omitido: The size of tensor a (560) must match the size of tensor b (512) at non-singleton dimension 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at mrm8488/bert-spanish-cased-finetuned-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
            "  warnings.warn(\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0002.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0004.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0006.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0008.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0010.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0012.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0014.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0016.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0018.txt\n",
            "Error procesando página 19, fragmento omitido: The size of tensor a (513) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0020.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0022.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0024.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0026.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0028.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0030.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0032.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0034.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0036.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0038.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0040.txt\n",
            "Error procesando página 42, fragmento omitido: The size of tensor a (580) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0044.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0046.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0048.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0050.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0052.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0054.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0056.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0058.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0060.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0062.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0064.txt\n",
            "Error procesando página 65, fragmento omitido: The size of tensor a (623) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2011libro1_carpetatxt/gen2011libro1_page-0066.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at mrm8488/bert-spanish-cased-finetuned-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0002.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0004.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0006.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0008.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0010.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0012.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0014.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0016.txt\n",
            "Error procesando página 17, fragmento omitido: The size of tensor a (577) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0018.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0020.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0022.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0024.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0026.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0028.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0030.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0032.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0034.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0036.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0038.txt\n",
            "Error procesando página 39, fragmento omitido: The size of tensor a (530) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0040.txt\n",
            "Error procesando página 41, fragmento omitido: The size of tensor a (591) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0042.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0044.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0046.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0048.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0050.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0052.txt\n",
            "Error procesando página 53, fragmento omitido: The size of tensor a (560) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0054.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0056.txt\n",
            "Error procesando página 57, fragmento omitido: The size of tensor a (516) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0058.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0060.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0062.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0064.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0066.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0068.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2014libro1_carpetatxt/gen2014libro1_page-0070.txt\n",
            "Error procesando página 71, fragmento omitido: The size of tensor a (584) must match the size of tensor b (512) at non-singleton dimension 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at mrm8488/bert-spanish-cased-finetuned-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0002.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0004.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0006.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0008.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0010.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0012.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0014.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0016.txt\n",
            "Error procesando página 17, fragmento omitido: The size of tensor a (577) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0018.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0020.txt\n",
            "Error procesando página 21, fragmento omitido: The size of tensor a (566) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0022.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0024.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0026.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0028.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0030.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0032.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0034.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0036.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0038.txt\n",
            "Error procesando página 39, fragmento omitido: The size of tensor a (532) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0040.txt\n",
            "Error procesando página 41, fragmento omitido: The size of tensor a (599) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0042.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0044.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0046.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0048.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0050.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0052.txt\n",
            "Error procesando página 53, fragmento omitido: The size of tensor a (569) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0054.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0056.txt\n",
            "Error procesando página 57, fragmento omitido: The size of tensor a (519) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0058.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0060.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0062.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0064.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0066.txt\n",
            "Archivo no encontrado: /content/drive/MyDrive/Proyecto_Text2KG/Texto/gen2019libro1_carpetatxt/gen2019libro1_page-0070.txt\n",
            "Error procesando página 71, fragmento omitido: The size of tensor a (595) must match the size of tensor b (512) at non-singleton dimension 1\n"
          ]
        }
      ]
    }
  ]
}